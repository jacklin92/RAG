{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63375eab",
   "metadata": {},
   "source": [
    "### 任務類型\n",
    "檢索增強生成 (Retrieval-Augmented Generation, RAG)\n",
    "\n",
    "### 任務目標\n",
    "建立一個本地端的 AI Agent。它會讀取collect.ipynb產生的 vault.txt 檔案，使用 Ollama 上的本地嵌入模型 (nomic-embed-text) 將文件內容轉換為向量，並儲存在 Chroma 向量資料庫中。接著，它啟動一個互動式聊天介面，讓使用者提問，腳本會先從向量資料庫中檢索相關的郵件內容片段，再將這些內容連同問題一起交給本地的 LLM (llama3.1:8b) 來生成回答。\n",
    "\n",
    "### 資料集\n",
    "collect.ipynb產生的 vault.txt檔案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a198f99",
   "metadata": {},
   "source": [
    "### 步驟 1｜匯入所有需要的函式庫\n",
    "**TextLoader**: 用於載入 vault.txt。\n",
    "\n",
    "**OllamaEmbeddings**: 用於連接本地 Ollama 的嵌入模型。\n",
    "\n",
    "**Chroma**: 用於本地向量資料庫。\n",
    "\n",
    "**ChatOllama**: 用於連接本地 Ollama 的大型語言模型 (LLM)。\n",
    "\n",
    "**RecursiveCharacterTextSplitter**: 用於將載入的 vault.txt 內容切塊。\n",
    "\n",
    "**ChatPromptTemplate, RunnableParallel, StrOutputParser** 等：LangChain 運作鏈 (LCEL) 的核心組件，用於構建 RAG 流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d298f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import logging\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma \n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380cedc3",
   "metadata": {},
   "source": [
    "### 步驟 1.5｜美化輸出增加可讀性\n",
    "定義 ANSI 顏色代碼，用於在終端機中以不同顏色(例如 NEON_GREEN)印出 AI 的回應，增加可讀性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eade6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colors\n",
    "PINK = \"\\033[95m\"\n",
    "CYAN = \"\\033[96m\"\n",
    "NEON_GREEN = \"\\033[92m\"\n",
    "RESET_COLOR = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695c7d0",
   "metadata": {},
   "source": [
    "### 步驟 2｜設定logging的格式\n",
    "**load_config(config_file)**:讀取config.yaml中的設定參數。\n",
    "\n",
    "**logging.basicConfig()**:設定logging的基本組態，讓記錄包含時間戳、等級和訊息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    logging.info(f\"Loading configuration from '{config_file}'...\")\n",
    "    try:\n",
    "        with open(config_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            return yaml.safe_load(file)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Configuration file '{config_file}' not found.\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading configuration: {e}\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684fecd",
   "metadata": {},
   "source": [
    "### 步驟 3｜設計內容向量的產生流程\n",
    "**lload_or_create_vector_store()**:\n",
    "\n",
    "1. 載入文件: 使用 TextLoader 載入 vault.txt 並用 RecursiveCharacterTextSplitter 切塊。\n",
    "\n",
    "2. 檢查快取: 檢查 vector_store_path（./chroma_db_notebook）是否存在。\n",
    "\n",
    "3. 建立新 DB: 如果不存在，它會呼叫 Chroma.from_documents，這會花費一些時間來為所有文件塊生成嵌入向量，並將它們儲存到磁碟。\n",
    "\n",
    "4. 載入舊 DB (並更新): 如果已存在，它會直接載入。接著，它會執行一個「增量更新」檢查：它比較 vault.txt 中的「當前」文件塊和資料庫中「已存在」的文件塊。如果發現 vault.txt 中有新的內容，它只會將「新」的文件塊（new_docs_to_add）加入到資料庫中，而不是每次都重建整個資料庫。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_vector_store(config, embeddings_model, clear_cache=False):\n",
    "    vector_store_path = config[\"vector_store_path\"]\n",
    "\n",
    "    if clear_cache and os.path.exists(vector_store_path):\n",
    "        logging.info(f\"Clearing cache at '{vector_store_path}'...\")\n",
    "        shutil.rmtree(vector_store_path)\n",
    "\n",
    "    vault_file = config[\"vault_file\"]\n",
    "    if not os.path.exists(vault_file):\n",
    "        logging.error(f\"Vault file '{vault_file}' not found.\")\n",
    "        if not os.path.exists(vector_store_path):\n",
    "            logging.error(\"No vault file and no existing vector store. Exiting.\")\n",
    "            exit(1)\n",
    "\n",
    "        logging.warning(\n",
    "            \"Vault file not found, loading existing vector store without updates.\"\n",
    "        )\n",
    "        return Chroma(\n",
    "            persist_directory=vector_store_path, embedding_function=embeddings_model\n",
    "        )\n",
    "\n",
    "    logging.info(f\"Loading documents from '{vault_file}'...\")\n",
    "    loader = TextLoader(vault_file, encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    logging.info(\"Splitting documents recursively...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    current_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    if not os.path.exists(vector_store_path):\n",
    "        logging.info(\"Creating new vector store...\")\n",
    "\n",
    "        if not current_docs:\n",
    "            logging.error(\n",
    "                \"No document chunks found after splitting. Is 'vault.txt' empty?\"\n",
    "            )\n",
    "            exit(1)\n",
    "\n",
    "        logging.info(\n",
    "            f\"Found {len(current_docs)} document chunks. Generating embeddings...\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            db = Chroma.from_documents(\n",
    "                current_docs, embeddings_model, persist_directory=vector_store_path\n",
    "            )\n",
    "            logging.info(f\"Vector store created and saved to '{vector_store_path}'.\")\n",
    "            return db\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Failed to create embeddings or vector store: {e}\", exc_info=True\n",
    "            )\n",
    "            exit(1)\n",
    "    else:\n",
    "        logging.info(f\"Loading existing vector store from '{vector_store_path}'...\")\n",
    "        try:\n",
    "            db = Chroma(\n",
    "                persist_directory=vector_store_path, embedding_function=embeddings_model\n",
    "            )\n",
    "\n",
    "            logging.info(\"Checking for document updates...\")\n",
    "            existing_data = db.get(include=[\"documents\"])\n",
    "            existing_docs_content = set(existing_data[\"documents\"])\n",
    "\n",
    "            if not existing_docs_content:\n",
    "                logging.warning(\"Existing vector store is empty. Re-populating...\")\n",
    "                db.add_documents(current_docs)\n",
    "                logging.info(\"Vector store re-populated.\")\n",
    "                return db\n",
    "            new_docs_to_add = []\n",
    "            for doc in current_docs:\n",
    "                if doc.page_content not in existing_docs_content:\n",
    "                    new_docs_to_add.append(doc)\n",
    "            if new_docs_to_add:\n",
    "                logging.info(\n",
    "                    f\"Found {len(new_docs_to_add)} new document chunks. Adding to vector store...\"\n",
    "                )\n",
    "                db.add_documents(new_docs_to_add)\n",
    "                logging.info(\"Vector store successfully updated.\")\n",
    "            else:\n",
    "                logging.info(\"Vector store is already up-to-date.\")\n",
    "\n",
    "            return db\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Failed to load or update vector store from {vector_store_path}: {e}\",\n",
    "                exc_info=True,\n",
    "            )\n",
    "            exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53456eb",
   "metadata": {},
   "source": [
    "### 步驟 4｜定義主程式\n",
    "**format_docs()**:\n",
    "將檢索到的文件列表轉為單一字串。\n",
    "**main()**:\n",
    "1. 初始化: 載入設定檔、初始化 OllamaEmbeddings 和 ChatOllama 模型。\n",
    "\n",
    "2. 載入/更新 DB: 呼叫 load_or_create_vector_store 來準備好向量資料庫和檢索器 (retriever)。\n",
    "\n",
    "3. 定義提示詞: 建立 ChatPromptTemplate，它包含了系統訊息、聊天歷史 (history)，以及一個組合了「上下文 (context)」和「問題 (question)」的區塊。\n",
    "\n",
    "4. 建立 RAG 鏈: 這是最關鍵的部分。使用 RunnableParallel 來定義 RAG 流程：\n",
    "\n",
    "    - 當使用者輸入 question 時，系統「同時」執行三件事：\n",
    "\n",
    "        - context: 將 question 傳給 retriever 進行檢索，並用 format_docs 格式化。\n",
    "\n",
    "        - question: 將 question 原樣傳遞下去。\n",
    "\n",
    "        - history: 將 history 變數原樣傳遞下去。\n",
    "\n",
    "    - 將這三樣東西 (context, question, history) 一起餵給 prompt。\n",
    "\n",
    "    - 將格式化後的提示詞交給 llm 生成答案。\n",
    "\n",
    "    - 最後用 StrOutputParser 取得純文字回應。\n",
    "\n",
    "5. 啟動聊天: 進入 while True 迴圈，等待使用者輸入。\n",
    "\n",
    "6. 維護歷史: 每次問答後，將使用者的 HumanMessage 和 AI 的 AIMessage 存入 chat_history 列表，以便在下次提問時提供上下文記憶。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfe7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    CONFIG_FILE = \"configwj.yaml\"\n",
    "    CLEAR_CACHE = False\n",
    "\n",
    "    config = load_config(CONFIG_FILE)\n",
    "    \n",
    "\n",
    "    embeddings_model = OllamaEmbeddings(model=config[\"embed_model\"])\n",
    "\n",
    "    llm = ChatOllama(model=config[\"ollama_model\"])\n",
    "\n",
    "    db = load_or_create_vector_store(config, embeddings_model, CLEAR_CACHE)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": config[\"top_k\"]})\n",
    "\n",
    "    chat_history = []\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", config[\"system_message\"]),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Based on the following context from my documents:\\n\"\n",
    "                \"--- CONTEXT ---\\n\"\n",
    "                \"{context}\\n\"\n",
    "                \"--- END CONTEXT ---\\n\\n\"\n",
    "                \"My question is: {question}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    rag_chain = (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                \"context\": (lambda x: x[\"question\"]) | retriever | format_docs,\n",
    "                \"question\": (lambda x: x[\"question\"]),\n",
    "                \"history\": (lambda x: x[\"history\"]),\n",
    "            }\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    logging.info(\n",
    "        f\"Chatbot initialized with model '{config['ollama_model']}'. Type 'quit' to exit.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\n",
    "                \"Ask a question about your documents: \"\n",
    "            )\n",
    "            if user_input.lower() == \"quit\":\n",
    "                break\n",
    "\n",
    "            if not user_input.strip():\n",
    "                continue\n",
    "\n",
    "            inputs = {\"question\": user_input, \"history\": chat_history}\n",
    "            response = rag_chain.invoke(inputs)\n",
    "            print(NEON_GREEN + \"Response: \\n\\n\" + response + RESET_COLOR)\n",
    "\n",
    "            chat_history.append(HumanMessage(content=user_input))\n",
    "            chat_history.append(AIMessage(content=response))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during chat: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45127169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 23:30:56,148 - INFO - Loading configuration from 'configwj.yaml'...\n",
      "2025-10-28 23:30:56,168 - INFO - Loading documents from 'vault.txt'...\n",
      "2025-10-28 23:30:56,195 - INFO - Splitting documents recursively...\n",
      "2025-10-28 23:30:56,201 - INFO - Loading existing vector store from './chroma_db_notebook'...\n",
      "2025-10-28 23:30:56,347 - INFO - Checking for document updates...\n",
      "2025-10-28 23:30:56,354 - INFO - Vector store is already up-to-date.\n",
      "2025-10-28 23:30:56,355 - INFO - Chatbot initialized with model 'llama3.1:8b'. Type 'quit' to exit.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
